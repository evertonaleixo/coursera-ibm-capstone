{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n <a href=\"http://cocl.us/pytorch_link_top\"><img src = \"http://cocl.us/Pytorch_top\" width = 950, align = \"center\">\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<img src = \"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width = 200, align = \"center\">\n\n<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h3>Objective for this Notebook<h3>    \n<h5> 1. Learn Convolutional Neral Network</h5>\n<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Table of Contents\n\nIn this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines \n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n<li><a href=\"#ref0\">Helper functions </a></li>\n\n<li><a href=\"#ref1\"> Prepare Data </a></li>\n<li><a href=\"#ref2\">Convolutional Neral Network </a></li>\n<li><a href=\"#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n<li><a href=\"#ref4\">Analyse Results</a></li>\n\n<br>\n<p></p>\nEstimated Time Needed: <strong>25 min</strong>\n</div>\n\n<hr>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"ref0\"></a>\n\n<h2 align=center>Helper functions </h2>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "#!conda install -y torchvision\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7fbca6832e90>"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "torch.manual_seed(4)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "function to plot out the parameters of the Convolutional layers  \n"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n    \n        if in_index>n_in-1:\n            out_index=out_index+1\n            in_index=0\n              \n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<code>show_data</code>: plot out data sample\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "create some toy data \n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n        \n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset>0:\n        \n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n        \n            if n<=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n>N_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n           \n        \n        \n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n    \n    \n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i< n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Utility function for computing output of convolutions\ntakes a tuple of (h,w) and returns a tuple of (h,w)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "\ndef conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"ref1\"></a>\n\n<h2 align=center>Prepare Data </h2> \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Load the training dataset with 10000 samples \n"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "N_images=10000\ntrain_dataset=Data(N_images=N_images)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Load the testing dataset\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<__main__.Data at 0x7fbc8cb58410>"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "we can see the data type is long \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Data Visualization\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can print out the third label \n"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMI0lEQVR4nO3dbYyldXnH8e/PXakCEmhMFRYUSIitpaWYjQVpGgM22UYivKgpJrSWNNkXrRW1rbEmjU2aJiY1DaTpQ1aKJUIgZiWWGIM2lmjfdMMCpbAsthQVVpYHIwL2DRKuvpiDnZ3O7MM595n7Xq7vJ9nMzNkz97l2hi///3mYe1JVSHr1e83YA0jaHMYuNWHsUhPGLjVh7FITxi41YexSE8auY5bkp5LcmOT5JE8m+djYM+nIto49gI5LfwacB7wVeDNwV5KHqurOUafSYbmyN5Pkj5N8cc1lf53kumM4zG8Df15Vz1bVfuCzwO8MOKaWwNj7uRnYkeRUgCRbgd8EPp/kb5P8cIM//zG7/mnAGcD9q455P/Dzm/zv0DFyG99MVR1M8k3g/aysyDuA71fVPcA9wO8d4RAnz94+t+qy54A3DD2rhuXK3tNNwNWz968GPn8Mn/uj2dtTVl12CvDCAHNpiYy9py8Bv5jkfOBy4BaAJH+f5Ecb/NkHUFXPAgeBC1Yd7wJg3yb/G3SM4o+49pTks8Avs7KFv/QYP/fTwMXAlcCbgLuAa3w0ftpc2fu6CfgFjm0L/4pPAf8NfBf4BvCXhj59ruxNJXkL8DDw5qp6fux5tHyu7A0leQ3wMeA2Q+/Dp96aSXIS8BQrW/AdI4+jTeQ2XmrCbbzUxKZu45O4jZCWrKqy3uWu7FITxi41YexSE8YuNWHsUhPGLjWxUOxJdiT5VpJHknxiqKEkDW/uV9Al2QL8J/BrwAHgbuADVfXQYT7H59mlJVvG8+zvBB6pqker6kXgNuCKBY4naYkWiX0b8Piqjw/MLjtEkp1J9ibZu8BtSVrQIi+XXW+r8P+26VW1C9gFbuOlMS2ysh8Azlr18ZnAE4uNI2lZFon9buC8JOckOQG4CrhjmLEkDW3ubXxVvZTkQ8BXgS3AjVXlGUalidrUk1d4n11aPn/EVWrO2KUmjF1qwtilJjyV9HGi21mAk3UfY9ICXNmlJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJjwHXVNDn+Ot2znyjkeu7FITxi41YexSE8YuNWHsUhPGLjUxd+xJzkpyV5L9SfYluXbIwSQNK/M+P5rkdOD0qro3yRuAe4Arq+qhw3yOT8bOaejnsaf+PLu/621+VbXuF2/ulb2qDlbVvbP3XwD2A9vmPZ6k5RrkFXRJzgYuBPas83c7gZ1D3I6k+c29jf/JAZKTgW8Af1FVtx/hum7j5zT1bfLU5+tk8G08QJLXAl8EbjlS6JLGtcgDdAFuAn5QVR85ys9xZZ/T1FfOqc/XyUYr+yKx/wrwr8ADwMuziz9ZVV85zOcY+5ymHtPU5+tk8NjnYezzm3pMU5+vk6XcZ5d0/DB2qQljl5rwtFRNeRqpflzZpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWpi4diTbElyX5IvDzGQpOUYYmW/Ftg/wHEkLdFCsSc5E3gvcMMw40halkVX9uuAjwMvb3SFJDuT7E2yd8HbkrSAuWNPcjnwdFXdc7jrVdWuqtpeVdvnvS1Ji1tkZb8EeF+S7wC3AZcmuXmQqSQNLlW1+EGSdwN/VFWXH+F6i99YU0N8n44nScYe4bhVVet+8XyeXWpikJX9qG/MlX1uruw6Wq7sUnPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNbF17AE0jqF/CUO3X2JxPHJll5owdqkJY5eaMHapCWOXmjB2qYmFYk9yapLdSR5Osj/JxUMNJmlYiz7Pfj1wZ1X9RpITgBMHmEnSEmTeF0MkOQW4Hzi3jvIgSXzlxZyGftHK1F9UM/R8nVTVul+8Rbbx5wLPAJ9Lcl+SG5KctPZKSXYm2Ztk7wK3JWlBi6zs24F/Ay6pqj1Jrgeer6o/PcznuLLPaeor59Tn62QZK/sB4EBV7Zl9vBt4xwLHk7REc8deVU8Cjyd52+yiy4CHBplK0uDm3sYDJPkl4AbgBOBR4JqqevYw13cbP6epb5OnPl8nG23jF4r9WBn7/KYe09Tn62QZ99klHUeMXWrC2KUmjF1qwnPQNeU54/pxZZeaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5ea8Bx0GoS/1GH6XNmlJoxdasLYpSaMXWrC2KUmFoo9yUeT7EvyYJJbk7xuqMEkDWvu2JNsAz4MbK+q84EtwFVDDSZpWItu47cCr0+yFTgReGLxkSQtw9yxV9X3gM8AjwEHgeeq6mtrr5dkZ5K9SfbOP6akRS2yjT8NuAI4BzgDOCnJ1WuvV1W7qmp7VW2ff0xJi1pkG/8e4NtV9UxV/Ri4HXjXMGNJGtoisT8GXJTkxKy8MPoyYP8wY0ka2iL32fcAu4F7gQdmx9o10FySBpaq2rwbSzbvxl5lNvP7NA9/6m06qmrdb4avoJOaMHapCWOXmjB2qQlPS3WcGPoBsKEf8Bv6eD7gNzxXdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJz0HXlOd468eVXWrC2KUmjF1qwtilJoxdasLYpSaOGHuSG5M8neTBVZf9dJJ/TvJfs7enLXdMSYs6mpX9H4Eday77BPD1qjoP+PrsY0kTdsTYq+qbwA/WXHwFcNPs/ZuAKweeS9LA5n0F3Zuq6iBAVR1M8jMbXTHJTmDnnLcjaSBLf7lsVe0CdgEkGfb3+ko6avM+Gv9UktMBZm+fHm4kScswb+x3AB+cvf9B4J+GGUfSsqTq8DvrJLcC7wbeCDwFfAr4EvAF4C3AY8D7q2rtg3jrHcttvLRkVbXujzQeMfYhGbu0fBvF7ivopCaMXWrC2KUmjF1qYrPPQfd94LtHcb03zq47RVOeDaY935Rng1fHfG/d6C829dH4o5Vkb1VtH3uO9Ux5Npj2fFOeDV7987mNl5owdqmJqca+a+wBDmPKs8G055vybPAqn2+S99klDW+qK7ukgRm71MSkYk+yI8m3kjySZFLntUtyVpK7kuxPsi/JtWPPtFaSLUnuS/LlsWdZK8mpSXYneXj2Nbx47JlekeSjs+/pg0luTfK6kedZykleJxN7ki3A3wC/Drwd+ECSt4871SFeAv6wqn4OuAj4/YnNB3AtsH/sITZwPXBnVf0scAETmTPJNuDDwPaqOh/YAlw17lTLOcnrZGIH3gk8UlWPVtWLwG2snNhyEqrqYFXdO3v/BVb+Y9027lT/J8mZwHuBG8aeZa0kpwC/CvwDQFW9WFU/HHeqQ2wFXp9kK3Ai8MSYwyzrJK9Tin0b8Piqjw8woZhWS3I2cCGwZ9xJDnEd8HHg5bEHWce5wDPA52Z3M25IctLYQwFU1feAz7ByEpaDwHNV9bVxp1rXISd5BTY8yetGphT7ej9wP7nnBZOcDHwR+EhVPT/2PABJLgeerqp7xp5lA1uBdwB/V1UXAv/DRH7XwOy+7xXAOcAZwElJrh53quWYUuwHgLNWfXwmI2+n1kryWlZCv6Wqbh97nlUuAd6X5Dus3P25NMnN4450iAPAgap6ZSe0m5X4p+A9wLer6pmq+jFwO/CukWdaz8IneZ1S7HcD5yU5J8kJrDxIcsfIM/1EkrByn3N/Vf3V2POsVlV/UlVnVtXZrHzd/qWqJrM6VdWTwONJ3ja76DLgoRFHWu0x4KIkJ86+x5cxkQcP11j4JK+b/SOuG6qql5J8CPgqK4+I3lhV+0Yea7VLgN8CHkjy77PLPllVXxlxpuPJHwC3zP5H/ihwzcjzAFBVe5LsBu5l5RmX+xj5ZbOrT/Ka5AArJ3n9NPCFJL/L7CSvx3xcXy4r9TClbbykJTJ2qQljl5owdqkJY5eaMHapCWOXmvhfKPVFJGG+WzsAAAAASUVORK5CYII=\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "show_data(train_dataset,0)"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "show_data(train_dataset,N_images//2+2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "we can plot the 3rd  sample \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"ref3\"></a>\n\n### Build a Convolutional Neral Network Class\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The input image is 11 x11, the following will change the size of the activations:\n\n<ul>\n<il>convolutional layer</il> \n</ul>\n<ul>\n<il>max pooling layer</il> \n</ul>\n<ul>\n<il>convolutional layer </il>\n</ul>\n<ul>\n<il>max pooling layer </il>\n</ul>\n\nwith the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\nWe use the following  lines of code to change the image before we get tot he fully connected layer \n"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(10, 10)\n(9, 9)\n(8, 8)\n(7, 7)\n"
                }
            ],
            "source": "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n    \n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n        \n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"ref3\"></a>\n\n<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "model=CNN(2,1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "we can see the model parameters with the object \n"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "CNN(\n  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=49, out_features=2, bias=True)\n)"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAADrCAYAAABNVDkBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADu0lEQVR4nO3dPW4TURRA4Xv5kQJBIkVCQeMy7jMtYhnswItgKd4Fm6Ayfdwgyki4cJGC7lLQBMkwGunNT3LO144l39GR3yjSe5msqtDT9mzuATQ+IwMYGcDIAEYGMDLAi74PZOYmIjYREednZzfr1Wr0oTTcj7u7OByPeepaDvk7uVuva7fdNhtM7XSbTexub09GdrkGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAL2RM3OTmbvM3P08HqeYSY31Rq6qbVV1VdVdXVxMMZMac7kGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGaD39QQPfdvfR378OtYsM/g09wANvfznlUHvoIhwc/1jNOgERcT5FDOpMZ/JAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcATFACeoABwuQYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZIKvq/x/4a3N9XEfEfuyhIuIyIg4TfM8UprqXVVVdnbrQG3kOmbn7s5n/8VvCvbhcAxgZYKmRt3MP0NDs97LIZ7LaWuovWQ0ZGcDIAEYGMDLAwP8Z8vwm4s3II01p0JuFF+4+qn7lqSuD/oTKvKiID83Gmt+7uQdo6EtUHU5GdrkGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAANPULwaeRyNwRMUT4YnKNCMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGGLTv+n1mbfo/9mis5x6goc8R8b3q5L7rQSco3radSxPpXa6raltVXVV1r6eYSM35TAYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwM31AG6uB3C5BjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyAC976B4eIIiIq4jYj/2UBFxGRGHCb5nClPdy6qqrk5dGPSikalk5q6qurnnaGEJ9+JyDWBkgKVG3s49QEOz38sin8lqa6m/ZDVkZAAjAxgZwMgAvwF376R/TskBAQAAAABJRU5ErkJggg==\n",
                        "text/plain": "<Figure size 432x288 with 2 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "\nplot_channels(model.state_dict()['cnn1.weight'])\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Loss function \n"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAACqCAYAAACTZZUqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEFUlEQVR4nO3asYpdVRiG4X/JFDISBsLYBNMkiBamMecybLyJfQfeh6VMmTvILZj2dE5pKwQiQ2wC2vwWWpzAxJ1tztlfzpzn6QZ2WB/D4iUsZnR3AbC+T9IDAE6VAAOECDBAiAADhAgwQIgAA4SczX0wxpiqavrnp0+fVn1x4El3yav0gCPzprr/GmuctHuvPzs/f/r1o0drHHsn/Hl9nZ5wVH6rqpvuW+/1WPJ3wGN82VU/7mvXCfgpPeDIvKju16sEeNfmyZPePn++9rFH69fHj9MTjsr3VfXLOwLsCQIgRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCZgM8xpjGGNsxxrbqjzU2wcHt3utXNzfpOZyo2QB391V3b7p7U3WxxiY4uN17/fn9++k5nChPEAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACGju9/74wdj9HTAMXfNmx/e/3dL1bNnm3r5cjvWPvfbMfrntQ89Yvfqu/SEI/Oiul/feq/P5v7pGGOqqqmq6mLPsyBl914/DG/hdM0+QXT3VXdvuntzvsYiWMHuvb5Mj+FkeQMGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAkLO5D8YYU1VNVVUXB58D69i91w/DWzhds/8D7u6r7t509+Z8jUWwgt17fZkew8nyBAEQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAho7v/+4Mxpqqa/v3xm6q6PvSo/+Gyqn5Pj7iFXct81d331jjIvf4gdi3zzns9G+C3Ph5j292bvc3aE7uWsevjOHeOXcsc4y5PEAAhAgwQsjTAVwdZ8eHsWsauj+PcOXYtc3S7Fr0BA7A/niAAQgQYIESAAUIEGCBEgAFC/gYhD8L73BBEIAAAAABJRU5ErkJggg==\n",
                        "text/plain": "<Figure size 432x288 with 2 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plot_channels(model.state_dict()['cnn2.weight'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Define the loss function \n"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "criterion=nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " optimizer class \n"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Define the optimizer class \n"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "\ntrain_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n      \n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n        \n        \n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n        \n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n    \n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### <a id=\"ref3\"></a>\n\n<h2 align=center>Analyse Results</h2> \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Plot the loss and accuracy on the validation data:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "View the results of the parameters for the Convolutional layers \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model.state_dict()['cnn1.weight']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plot_channels(model.state_dict()['cnn1.weight'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model.state_dict()['cnn1.weight']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plot_channels(model.state_dict()['cnn2.weight'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Consider the following sample \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "show_data(train_dataset,N_images//2+2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Determine the activations \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Plot them out\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "we save the output of the activation after flattening  \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "out1=out[4][0].detach().numpy()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "we can do the same for a sample  where y=0 \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### About the Authors:\n\n[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork-20647811&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n\nOther contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork-20647811&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ) \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n| 2020-09-23        | 2.0     | Srishti    | Migrated Lab to Markdown and added to course repo in GitLab |\n\n<hr>\n\n## <h3 align=\"center\"> \u00a9 IBM Corporation 2020. All rights reserved. <h3/>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "  <hr>\nCopyright &copy; 2018 [cognitiveclass.ai](cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}